<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Axect&#39;s Blog</title>
    <link>https://axect.github.io/en/</link>
    <description>Recent content on Axect&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Mon, 06 Nov 2023 19:03:00 +0900</lastBuildDate>
    <atom:link href="https://axect.github.io/en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://axect.github.io/en/about/</link>
      <pubDate>Mon, 06 Nov 2023 19:03:00 +0900</pubDate>
      <guid>https://axect.github.io/en/about/</guid>
      <description>&lt;p&gt;About &lt;a href=&#34;https://github.com/Axect&#34;&gt;&lt;strong&gt;Tae Geun Kim (Axect)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;i-am&#34;&gt;I am&lt;/h2&gt;&#xA;&lt;p&gt;Graduate student &amp;amp; Rustacean&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;M.S. &amp;amp; Ph.D. Integrated: Department of Physics, Yonsei University (2017 ~ )&lt;/li&gt;&#xA;&lt;li&gt;B.S.: Department of Astronomy, Yonsei University (2012 ~ 2017)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;research-area&#34;&gt;Research Area&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Astroparticle Physics&lt;/li&gt;&#xA;&lt;li&gt;Dark matter &amp;amp; BSM&lt;/li&gt;&#xA;&lt;li&gt;Scientific computation &amp;amp; Machine learning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;skills&#34;&gt;Skills&lt;/h2&gt;&#xA;&lt;h3 id=&#34;mathematics&#34;&gt;Mathematics&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Functional Analysis&lt;/li&gt;&#xA;&lt;li&gt;Numerical Analysis&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Finite Difference Method&lt;/li&gt;&#xA;&lt;li&gt;Finite Element Method&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Differential Geometry&lt;/li&gt;&#xA;&lt;li&gt;Topology&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;physics&#34;&gt;Physics&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;General Relativity&lt;/li&gt;&#xA;&lt;li&gt;Quantum Field Theory&lt;/li&gt;&#xA;&lt;li&gt;Mathematical Physics&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Statistical Machine Learning&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Linear Regression (LASSO, Ridge)&lt;/li&gt;&#xA;&lt;li&gt;Logistic Regression&lt;/li&gt;&#xA;&lt;li&gt;Linear Discrimination&lt;/li&gt;&#xA;&lt;li&gt;Kernel Based Methods&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kernel Smoothing&lt;/li&gt;&#xA;&lt;li&gt;Kernel Density Estimation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Neural Network&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MLP, CNN, RNN (LSTM, GRU), Transformer, Mamba&lt;/li&gt;&#xA;&lt;li&gt;Operator learning &amp;amp; Neural ODE&lt;/li&gt;&#xA;&lt;li&gt;Bayesian Neural Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;programming&#34;&gt;Programming&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Main language: Rust, Julia, Python&lt;/li&gt;&#xA;&lt;li&gt;Sub languages: C/C++, Haskell&lt;/li&gt;&#xA;&lt;li&gt;Frameworks or Libraries&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Numerical: peroxide, BLAS, LAPACK, numpy, scipy&lt;/li&gt;&#xA;&lt;li&gt;Visualization: matplotlib, vegas, ggplot2, plotly&lt;/li&gt;&#xA;&lt;li&gt;Web: Django, Vue, Firebase, Surge, Hugo&lt;/li&gt;&#xA;&lt;li&gt;Machine Learning: PyTorch, JAX, Optax, Equinox, Wandb, Optuna, Candle, Tensorflow, Norse&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;project&#34;&gt;Project&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Peroxide&lt;/strong&gt;: Numerical library for Rust (Maintainer)&lt;/p&gt;</description>
    </item>
    <item>
      <title>ðŸ“Š Piecewise Rejection Sampling</title>
      <link>https://axect.github.io/en/posts/006_prs/</link>
      <pubDate>Fri, 18 Nov 2022 17:49:04 +0900</pubDate>
      <guid>https://axect.github.io/en/posts/006_prs/</guid>
      <description>&lt;figure&gt;&#xA;    &lt;img src=&#34;https://axect.github.io/posts/images/006_01_test_dist.png&#34;&#xA;         alt=&#34;Differential energy spectrum of ALPs from primordial black hole (PBH)${}^{[1]}$&#34;/&gt; &lt;figcaption style=&#34;text-align:center&#34;&gt;&#xA;            &lt;p&gt;Differential energy spectrum of ALPs from primordial black hole (PBH)&lt;a href=&#34;https://axect.github.io/en/#footnotes&#34;&gt;${}^{[1]}$&lt;/a&gt;&lt;/p&gt;&#xA;        &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;p&gt;Suppose you&amp;rsquo;re presented with an unnormalized probability density function graph such as the one shown. If you&amp;rsquo;re tasked with generating 10,000 data points that conform to this distribution, what would be your approach?&lt;/p&gt;&#xA;&lt;p&gt;Here are the two most commonly used methods to sample data from an arbitrary probability density function:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Inverse_transform_sampling&#34;&gt;Inverse Transform Sampling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rejection_sampling&#34;&gt;Rejection Sampling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Inverse Transform Sampling involves generating data points by calculating the cumulative distribution function (CDF) of the probability density function, deriving its inverse function, and then using this inverse function to produce data points. This method can be quite efficient, but if the exact form of the probability density function is unknownâ€”as in our caseâ€”it becomes challenging to apply&lt;a href=&#34;https://axect.github.io/en/posts/006_prs/#footnotes&#34;&gt;${}^{[2]}$&lt;/a&gt;. On the other hand, Rejection Sampling is versatile and can be utilized regardless of the probability density function&amp;rsquo;s form. Therefore, we&amp;rsquo;ll begin with Rejection Sampling.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ðŸ’” Decorrelation &#43; Deep learning = Generalization</title>
      <link>https://axect.github.io/en/posts/005_decov/</link>
      <pubDate>Sat, 29 Oct 2022 17:39:54 +0900</pubDate>
      <guid>https://axect.github.io/en/posts/005_decov/</guid>
      <description>&lt;figure&gt;&#xA;    &lt;img src=&#34;https://axect.github.io/posts/images/005_01_paper.png&#34;&#xA;         alt=&#34;arXiv: 1511.06068&#34;/&gt; &lt;figcaption style=&#34;text-align:center&#34;&gt;&#xA;            &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.06068&#34;&gt;arXiv: 1511.06068&lt;/a&gt;&lt;/p&gt;&#xA;        &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;p&gt;â€ƒâ€ƒThe most pervasive challenge in deep learning is &lt;span style=&#34;background-color: rgba(255, 255, 0, 0.534);&#34;&gt;&#xA;    &lt;b&gt;Overfitting&lt;/b&gt;&#xA;&lt;/span&gt;. This occurs when the dataset is small, and extensive training leads to a model that excels on training datasets but fails to generalize to validation datasets or real-world scenarios. To address this issue, various strategies have been developed. Historically, in statistics, regularization methods like Ridge and LASSO were employed, while deep learning has adopted approaches such as regularizing weights or applying different techniques to neural networks. These techniques encompass a range of methods.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
