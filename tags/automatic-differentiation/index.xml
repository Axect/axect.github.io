<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Automatic-Differentiation on Axect&#39;s Blog</title>
    <link>https://axect.github.io/tags/automatic-differentiation/</link>
    <description>Recent content in Automatic-Differentiation on Axect&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>kr</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Mon, 04 Dec 2023 15:38:04 +0900</lastBuildDate>
    <atom:link href="https://axect.github.io/tags/automatic-differentiation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>🤖 Rust와 미분하기 03: 정방향 자동 미분</title>
      <link>https://axect.github.io/posts/007_ad_3/</link>
      <pubDate>Mon, 04 Dec 2023 15:38:04 +0900</pubDate>
      <guid>https://axect.github.io/posts/007_ad_3/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;🔖 Automatic Differentiation Series&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;../002_ad_1&#34;&gt;💻 Numerical Differentiation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;../002_ad_2&#34;&gt;🖊️ Symbolic Differentiation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;../007_ad_3&#34;&gt;🤖 Automatic Differentiation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;딥러닝을 구현함에 있어서 가장 중요한 요소가 뭘까요?&#xA;물론 많은 학문으로 구성된 딥러닝의 특성상 모든 요소들이 다 중요하지만, 그 중에서도 특히 신경써야하는 요소가 있습니다.&#xA;이를 찾아내기 위해서 다음의 PyTorch 코드를 살펴봅시다.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sigmoid()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# x = ...&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# y = ...&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# criterion = ...&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;opt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; optim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;SGD(net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters(), lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;opt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_grad()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; criterion(net(x), y)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;opt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이를 아무런 딥러닝 프레임워크를 쓰지 않고 구현한다고 생각해봅시다.&#xA;일단 엄밀하게 같은 구현은 아니지만 &lt;code&gt;Linear&lt;/code&gt;와 &lt;code&gt;Sigmoid&lt;/code&gt; 함수 자체의 구현은 단순히 행렬곱과 벡터화된 sigmoid 함수를 사용하여 구현할 수 있으므로 &lt;code&gt;net(x)&lt;/code&gt;를 만드는 것은 어렵지 않습니다.&#xA;다음으로 여기선 &lt;code&gt;criterion&lt;/code&gt;이 무엇인지 명시하지는 않았지만 가장 기본적인 &lt;code&gt;MSE&lt;/code&gt;를 사용한다면 이 역시 간단합니다.&#xA;문제는 &lt;code&gt;opt&lt;/code&gt;부터 시작됩니다.&#xA;&lt;code&gt;SGD&lt;/code&gt;를 구현하려면, 어떤 &lt;code&gt;criterion&lt;/code&gt;이나 신경망 구조에서도 gradient 즉, 도함수를 구할 수 있어야 합니다.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
